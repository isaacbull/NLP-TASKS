{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEIDQXubAFJZyK9kRNy1kV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacbull/NLP-TASKS/blob/main/NLP_preprocessing_and_training_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a preprocessing and training pipeline for a NLP model..."
      ],
      "metadata": {
        "id": "33FwTYOrGg7A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V08S8oSXGyqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a gradient boosting model and gridsearchCV for Neural Architecture Search"
      ],
      "metadata": {
        "id": "fNi-pqa5GzXb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7C4Y5udGajo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import PorterStemmer\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "# Define the preprocessing class\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def remove_html_tags(self, text):\n",
        "        return BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    def remove_urls(self, text):\n",
        "        return re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        stop_words = set([\"this\", \"is\", \"a\", \"using\", \"and\", \"be\", \"can\", \"for\", \"have\", \"not\", \"with\"])\n",
        "        return [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    def stem_text(self, tokens):\n",
        "        return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    def lemmatize_text(self, tokens):\n",
        "        return [token.lemma_ for token in nlp(' '.join(tokens))]\n",
        "\n",
        "    def augment_text(self, text):\n",
        "        aug = naw.SynonymAug(aug_src='wordnet')\n",
        "        return aug.augment(text)\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        date_pattern = r'(\\d{1,2})(st|nd|rd|th)?\\s(January|February|March|April|May|June|July|August|September|October|November|December)\\s(\\d{4})'\n",
        "        date_replacement = r'\\4-\\3-\\1'\n",
        "        text = re.sub(date_pattern, date_replacement, text)\n",
        "        number_pattern = r'\\bone hundred\\b'\n",
        "        text = re.sub(number_pattern, '100', text)\n",
        "        return text\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return [' '.join(self.augment_text(self.lemmatize_text(self.stem_text(self.remove_stopwords(tokenizer.tokenize(self.normalize_text(self.remove_urls(self.remove_html_tags(text))))))))) for text in X]\n",
        "\n",
        "# Sample dataset\n",
        "texts = data.iloc[:, :-1].astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
        "labels = data.iloc[:, -1]\n",
        "\n",
        "# Preprocess texts\n",
        "preprocessor = TextPreprocessor()\n",
        "texts = preprocessor.fit_transform(texts)\n",
        "\n",
        "# Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "y = labels.values\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', TextPreprocessor()),\n",
        "    ('vectorizer', TfidfVectorizer()),\n",
        "    ('classifier', GradientBoostingClassifier())\n",
        "])\n",
        "\n",
        "# Define parameter grid for GridSearch\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__learning_rate': [0.1, 0.01],\n",
        "    'classifier__max_depth': [3, 4, 5]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model summary\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best model score: \", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a neural network and tuner from keras for NAS"
      ],
      "metadata": {
        "id": "I7JRZ2knHBMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from transformers import BertTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "import nlpaug.augmenter.word as naw\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from keras_tuner import HyperModel, RandomSearch\n",
        "\n",
        "# Load models\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def remove_html_tags(self, text):\n",
        "        return BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    def remove_urls(self, text):\n",
        "        return re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        stop_words = set([\"this\", \"is\", \"a\", \"using\", \"and\", \"be\", \"can\", \"for\", \"have\", \"not\", \"with\"])\n",
        "        return [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    def stem_text(self, tokens):\n",
        "        return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    def lemmatize_text(self, tokens):\n",
        "        return [token.lemma_ for token in nlp(' '.join(tokens))]\n",
        "\n",
        "    def augment_text(self, text):\n",
        "        aug = naw.SynonymAug(aug_src='wordnet')\n",
        "        return aug.augment(text)\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        date_pattern = r'(\\d{1,2})(st|nd|rd|th)?\\s(January|February|March|April|May|June|July|August|September|October|November|December)\\s(\\d{4})'\n",
        "        date_replacement = r'\\4-\\3-\\1'\n",
        "        text = re.sub(date_pattern, date_replacement, text)\n",
        "        number_pattern = r'\\bone hundred\\b'\n",
        "        text = re.sub(number_pattern, '100', text)\n",
        "        return text\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return [' '.join(self.augment_text(self.lemmatize_text(self.stem_text(self.remove_stopwords(tokenizer.tokenize(self.normalize_text(self.remove_urls(self.remove_html_tags(text))))))))) for text in X]\n",
        "\n",
        "class MyHyperModel(HyperModel):\n",
        "    def __init__(self, input_dim):\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu', input_shape=(self.input_dim,)))\n",
        "        model.add(Dropout(rate=hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "# Separate features and labels\n",
        "texts = data.iloc[:, :-1].astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
        "labels = data.iloc[:, -1]\n",
        "\n",
        "# Preprocess texts\n",
        "preprocessor = TextPreprocessor()\n",
        "processed_texts = preprocessor.fit_transform(texts)\n",
        "\n",
        "# Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(processed_texts).toarray()\n",
        "y = labels.values\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', TextPreprocessor()),  # Preprocessing step\n",
        "    ('vectorizer', TfidfVectorizer()),  # Vectorization step\n",
        "])\n",
        "\n",
        "# Preprocess and vectorize the training data\n",
        "X_train_transformed = pipeline.fit_transform(texts)\n",
        "input_dim = X_train_transformed.shape[1]\n",
        "\n",
        "# Hyperparameter tuning for neural network\n",
        "tuner = RandomSearch(MyHyperModel(input_dim=input_dim), objective='val_accuracy', max_trials=5)\n",
        "tuner.search(X_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Best model summary\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_model.summary()\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "y_pred = (best_model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "iSeW2MsgHHmd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}